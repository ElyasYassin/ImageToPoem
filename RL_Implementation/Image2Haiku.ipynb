{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Fine-Tuning a LLM for Haiku Structure </H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'IMG2POEM (Python 3.13.0)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "# Load your haiku dataset\n",
    "dataset = load_dataset('text', data_files={'train': 'haiku.txt'})\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=50)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Add labels for loss computation\n",
    "    return tokens\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 model\n",
    "model = GPT3LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Adjust GPT-2 to handle padding\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_haiku_model\",  # Save directory\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    "    per_device_train_batch_size=8,   # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",            # Log directory\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=5e-5,              # Adjust learning rate if needed\n",
    "    weight_decay=0.01,\n",
    "    do_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2_finetuned_haiku\")\n",
    "tokenizer.save_pretrained(\"./gpt2_finetuned_haiku\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elyas\\Anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Haiku: sunset over the mountain /  the sun sets over the valley /  the moon sets over me $\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_finetuned_haiku\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_finetuned_haiku\")\n",
    "\n",
    "def generate_haiku(prompt, model, tokenizer):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=50, \n",
    "        num_return_sequences=1, \n",
    "        temperature=0.7, \n",
    "        top_k=50\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"sunset over the mountain\"\n",
    "haiku = generate_haiku(prompt, model, tokenizer)\n",
    "print(\"Generated Haiku:\", haiku)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reward Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "nvec (counts) have to be positive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHaikuRefinerEnv_v0\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HaikuEnvironment\n\u001b[1;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mHaikuEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\OneDrive - The University of Colorado Denver\\Desktop\\Projects\\ImageToPoem\\RL_Implementation\\HaikuRefinerEnv_v0.py:28\u001b[0m, in \u001b[0;36mHaikuEnvironment.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_suggestions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of LM-generated actions\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Observation space (unchanged)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDict({\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscene\u001b[39m\u001b[38;5;124m\"\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msyllables_remaining\u001b[39m\u001b[38;5;124m\"\u001b[39m: gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mMultiDiscrete([\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m     31\u001b[0m })\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Action space: index of LM suggestion\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_suggestions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gymnasium\\spaces\\multi_discrete.py:70\u001b[0m, in \u001b[0;36mMultiDiscrete.__init__\u001b[1;34m(self, nvec, dtype, seed, start)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnvec\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnvec\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     69\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart and nvec (counts) should have the same shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnvec \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvec (counts) have to be positive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnvec\u001b[38;5;241m.\u001b[39mshape, dtype, seed)\n",
      "\u001b[1;31mAssertionError\u001b[0m: nvec (counts) have to be positive"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from HaikuRefinerEnv_v0 import HaikuEnvironment\n",
    "\n",
    "env = HaikuEnvironment()\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "print(\"Generated Haiku:\", env.haiku)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
