{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning Object Detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elyas\\AppData\\Local\\Temp\\ipykernel_7616\\3587342458.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  object_model = torch.load(\"model/object.params\")\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x12'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[63], line 4\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load pretrained model weights (for example, object detection model)\u001b[39;00m\n",
      "\u001b[1;32m----> 4\u001b[0m object_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel/object.params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      5\u001b[0m scene_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/scene.params\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m      6\u001b[0m sentiment_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/Sentiment.params\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\Anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\torch\\serialization.py:1384\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n",
      "\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m   1383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n",
      "\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Elyas\\Anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\torch\\serialization.py:1628\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n",
      "\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[0;32m   1623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1626\u001b[0m     )\n",
      "\u001b[1;32m-> 1628\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n",
      "\u001b[0;32m   1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x12'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load pretrained model weights (for example, object detection model)\n",
    "object_model = torch.load(\"model/object.params\")\n",
    "scene_model = torch.load(\"model/scene.params\")\n",
    "sentiment_model = torch.load(\"model/Sentiment.params\")\n",
    "\n",
    "# If these models are in a specific architecture like CNN, you can modify them as needed\n",
    "object_model.eval()\n",
    "scene_model.eval()\n",
    "sentiment_model.eval()\n",
    "\n",
    "# Example for image feature extraction\n",
    "def extract_features(image):\n",
    "    # Assuming the image is preprocessed and loaded as a PyTorch tensor\n",
    "    object_features = object_model(image)  # Get object features\n",
    "    scene_features = scene_model(image)    # Get scene features\n",
    "    sentiment_features = sentiment_model(image)  # Get sentiment features\n",
    "\n",
    "    return object_features, scene_features, sentiment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elyas\\Anaconda3\\envs\\Machinelearning\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load a pretrained ResNet or other model for image classification\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"  # You can replace this with other models like ResNet\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "# Load your image\n",
    "image = Image.open(\"../images/00000000.jpg\")\n",
    "# Preprocess the image for the model\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Get the feature embeddings from the model\n",
    "with torch.no_grad():\n",
    "    features = model.get_input_embeddings()(inputs['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects detected in the image: ['nature', 'nature']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "\n",
    "def get_conceptnet_objects(query, limit=50):\n",
    "    url = f\"http://api.conceptnet.io/c/en/{query}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [edge['end']['label'] for edge in data['edges'][:limit]]\n",
    "    return []\n",
    "\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load an image\n",
    "image_path = \"../images/00000010_(3).jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Example usage\n",
    "possible_objects = get_conceptnet_objects(\"landscape\") + get_conceptnet_objects(\"nature\")\n",
    "\n",
    "\n",
    "# Preprocess the image and text\n",
    "inputs = processor(text=possible_objects, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Get image and text features\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Compute cosine similarities between the image and each text label (object)\n",
    "image_features = outputs.image_embeds\n",
    "text_features = outputs.text_embeds\n",
    "\n",
    "# Normalize the features\n",
    "image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Compute similarity (cosine similarity)\n",
    "similarities = (image_features @ text_features.T).squeeze(0)\n",
    "\n",
    "# Get the top N most similar objects\n",
    "top_k = 2\n",
    "\n",
    "# Sort the similarities in descending order and get the top k indices\n",
    "top_k_indices = similarities.topk(top_k).indices\n",
    "\n",
    "# Print the top N objects\n",
    "top_objects = [possible_objects[i] for i in top_k_indices]\n",
    "print(f\"Objects detected in the image: {top_objects}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
