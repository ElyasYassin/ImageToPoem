{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import json\n",
    "from transformers import AutoProcessor, AutoTokenizer, BlipForConditionalGeneration, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import syllapy\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <b> NOTE: IF YOU JUST WANT TO TRY THE MODEL WITHOUT TRAINING RUN THE IMPORTS AND SKIP TO TESTING SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our dataset's source is: https://github.com/researchmm/img2poem/tree/master/data\n",
    "<p> It contains, in json format, thousands of image-poem sets\n",
    "<p> The problem is that some of the images don't work, so we have to go through all of them and make sure that they exist and create a processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data_path = \"datasets/processed_data.json\"\n",
    "data = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n",
    "        \n",
    "        encoding = self.processor(images=image, text=item[\"caption\"], padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> What model should we fine tune for this task ? </h2>\n",
    "<p> We chose to use the blip model because it has been pre trained on vision-language tasks, more specifically captioning:  <b><u> (https://huggingface.co/Salesforce/blip-image-captioning-base) </b> </u>\n",
    "<p> We thought: \"what if instead of captioning, it automatically produced the poems as the captions themselves?\"\n",
    "<p> And so, we started fine tuning the captioning model with our poem datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptioningDataset(data, processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "\n",
    "save_directory = \"models/finetuned_blip_captioning_model_new\"\n",
    "model.save_pretrained(save_directory)\n",
    "processor.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we're gonna see how well the model creates a line of a poem from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"models/finetuned_blip_captioning_model_new\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"models/finetuned_blip_captioning_model_new\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "image_path = \"image_test.jpg\"  \n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()} \n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "    **inputs, \n",
    "    max_length=7,      # Maximum length of the output\n",
    "    num_beams=1,        # Beam search for better results\n",
    "    no_repeat_ngram_size=1,  # Prevent repetition of n-grams\n",
    "    temperature=1,    # Controls randomness (lower is more deterministic)\n",
    "    top_k=3,           # Top-k sampling\n",
    "    top_p=0.95          # Nucleus sampling\n",
    ")\n",
    "\n",
    "caption = processor.batch_decode(generated_ids)[0]\n",
    "\n",
    "print(f\"Generated Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Poem Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now that we have a poem line generated from an image, our goal is to create a whole poem that follows the line\n",
    "<p> How do we do that?\n",
    "<p> We can fine tune a pretrained generative model like GPT-2 with haiku (small poem) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Training the model we had to find a lot of haiku data and preprocess them so that they were properly formatted for fine-tuning traning (check haikus.txt in datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": \"datasets/haiku.txt\"})\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '$'})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./poem_generator\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500, \n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model.save_pretrained(\"poem_generator\")\n",
    "tokenizer.save_pretrained(\"poem_generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_model = GPT2LMHeadModel.from_pretrained(\"models/gpt2_haiku_model/checkpoint-4000\")\n",
    "poem_tokenizer = GPT2Tokenizer.from_pretrained(\"models/gpt2_haiku_model/checkpoint-4000\")\n",
    "\n",
    "poem_model.to(device)\n",
    "\n",
    "poem_inputs = poem_tokenizer.encode(caption, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    poem_output = poem_model.generate(poem_inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=1, temperature=1, top_k=5)\n",
    "\n",
    "haiku = poem_tokenizer.decode(poem_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(haiku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Poem-To-Speech </h2>\n",
    "<p> We decided on using a fine-tuned version of Parler-TTS (https://github.com/huggingface/parler-tts) because it gave us the option to generate a poetic speaking style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\")\n",
    "\n",
    "prompt = haiku\n",
    "description = \"A female speaker with a slightly low-pitched, very expressive voice delivers her words at a normal  pace in a poetic but very slow manner with proper pauses while speaking inside a confined space with very clear audio\"\n",
    "\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> RL Training (Not working) </h2>\n",
    "<p> We were planning to use reinforcement learning to refine the poems for certain structures as shown in this paper: https://arxiv.org/abs/2102.04114\n",
    "<p> Unfortunately, we didn't have time to complete developping the suggestions part of the code (see HaikuRefinerEnv_v0) and start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from HaikuRefinerEnv_v0 import HaikuEnvironment\n",
    "\n",
    "env = HaikuEnvironment()\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "print(\"Generated Haiku:\", env.haiku)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
